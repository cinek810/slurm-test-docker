ClusterName=test2
MessageTimeout=30
SlurmdTimeout=5
AccountingStorageUser=slurm
#BurstBufferType=burst_buffer/datawarp
MemLimitEnforce = yes


MaxArraySize=2500


TreeWidth=8
TopologyPlugin=topology/tree
#TopologyParam=TopoOptional
#RoutePlugin=route/default

ControlMachine=localhost
#ControlAddr=localhost
SlurmUser=root
SlurmctldPort=6815
SlurmctldDebug=debug2
SlurmSchedLogLevel=0
SlurmdPort=6820
SlurmdDebug=debug4
SlurmdLogFile=/var/log/slurmd.log
SlurmctldLogFile=/var/log/slurmctld.log
DebugFlags=SelectType,Gres
AuthType=auth/munge
#AuthInfo=cred_expire=600
AccountingStorageEnforce=qos,limits
StateSaveLocation=/tmp/slurm_save
SlurmdSpoolDir=/tmp/slurmd_%n
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd%n.pid
ProctrackType=proctrack/cgroup
#ProctrackType=proctrack/cray_aries
#JobContainerType=job_container/cncu
TaskPlugin=task/affinity,task/cgroup
JobAcctGatherType=jobacct_gather/linux
FirstJobId=1
MaxJobCount=30000
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=slurmctl
SchedulerType=sched/backfill
#SchedulerParameters=defer,sched_interval=3600,sched_max_job_start=1,bf_interval=1,sched_min_interval=600,max_sched_time=6
#SchedulerType=sched/builtin
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
#SelectTypeParameters    = CR_Core
AllowSpecResourcesUsage=NO
PriorityFlags=NO_FAIR_TREE
PriorityType=priority/multifactor
PriorityDecayHalfLife=14-0
PriorityCalcPeriod=10
PriorityUsageResetPeriod=NOW
PriorityWeightFairshare=1500
PriorityWeightAge=500
MinJobAge=300
PriorityWeightPartition=1000
PriorityWeightJobSize=500
PriorityMaxAge=3-0
PriorityFavorSmall=NO
TmpFs=/tmp/
UsePAM=0
SwitchType=switch/none
MpiDefault=none
MpiParams=ports=12000-12999
ReturnToService=1
Prolog=/mnt/slurm/etc/prolog.sh
Epilog=/mnt/slurm/etc/epilog.sh
#PrologFlags=Contain
#FastSchedule=2
SlurmdParameters=config_overrides
PreemptMode=REQUEUE
JobRequeue=1
JobSubmitPlugins=lua
InactiveLimit=30
PreemptType=preempt/partition_prio
#PreemptExemptTime=00:01:00
#
#SallocDefaultCommand="srun  --pty --gres=NONE  --preserve-env $SHELL"

AccountingStorageTRES=gres/gpu

#DefMemPerCPU=504

EnforcePartLimits=ALL
#CliFilterPlugins=none
TrackWCKey=no

HealthCheckProgram=/mnt/slurm/etc/healthcheck.sh
HealthCheckInterval=60

#NodeName=DEFAULT	 CPUs=1  RealMemory=100 
NodeName=test01 NodeHostName=localhost CPUs=8 CoreSpecCount=0 Sockets=2 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=10000 State=UNKNOWN Weight=120 Port=30001 Gres=zero:1 TmpDisk=10 Features=cpu,fastio,amd,odd,first
NodeName=test02 NodeHostName=localhost CPUs=32 CoreSpecCount=0 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 RealMemory=382000 State=UNKNOWN Weight=120 Port=30002 Gres=gpu:tesla:4,tmpfs:400 Features=cpu,fastio,amd,first 
NodeName=test03 NodeHostName=localhost CPUs=8 CoreSpecCount=0 Sockets=2 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=5000 State=UNKNOWN Weight=120 Port=30003 TmpDisk=10 Features=cpu,slowio,amd,odd
NodeName=test04 NodeHostName=localhost CPUs=8 CoreSpecCount=0 Sockets=2 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=5000 State=UNKNOWN Weight=120 Port=30004 TmpDisk=10 Features=cpu,slowio,knl
NodeName=test05 NodeHostName=localhost CPUs=8 CoreSpecCount=0 Sockets=2 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=5000 State=UNKNOWN Weight=120 Port=30005 TmpDisk=10 Features=cpu,fastio,knl,odd
NodeName=test06 NodeHostName=localhost CPUs=8 CoreSpecCount=0 Sockets=2 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=5000 State=UNKNOWN Weight=120 Port=30006 TmpDisk=10 Features=fastio,xeon
NodeName=test07 NodeHostName=localhost CPUs=8 CoreSpecCount=0 Sockets=2 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=5000 State=UNKNOWN Weight=120 Port=30007 TmpDisk=10 Features=cpu,fastio,knl,odd
NodeName=test08 NodeHostName=localhost CPUs=32 CoreSpecCount=0 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 RealMemory=382000 State=UNKNOWN Weight=120 Port=30008 Gres=gpu:tesla:4,tmpfs:400 Features=cpu,fastio,xeon 

#NodeName=test[11-19] NodeHostName=localhost  Boards=1 SocketsPerBoard=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=486 State=UNKNOWN Weight=120 Port=[30011-30019]

#NERSC BUG NODES
#NodeName=nid[02534-02555,02557-02575,02580-02582,02666,02869-02891,02896-02904,03447-03465,03611-03626,03720-03727,03732-03744,03909-03915,03920-03929,04109-04111,04116-04128,04256-04284,04372-04399,04576-04600,04645-04683,04688-04747,04756-04811,04816-04825,04914-04936,04967-05000,05147-05168,05336-05391,05396-05440,05442-05451,05456-05469,05509-05515,05524-05537,05722-05753,06372-06389,06456-06475,06480-06543,06548-06603,06608-06614,06629-06661,06798-06799,06804-06819,06889-06927,06932-06949,06953-06974,07008-07034,07129-07160,07481-07498,07724-07755,07761-07823,07828-07883,07888-07943,08169-08183,08349-08395,08400-08424,08480-08498,08574-08588,09239-09291,09296-09310,09402-09417,09452-09471,09605-09615,09620-09625,10288-10311,10646-10684,10938-10955,10960-10964,10971-10998,11003-11023,11028-11030,11068-11083,11088-11095,11210-11211,11216-11237,11324-11339,11344-11346,11363-11407,11412-11467,11472-11505,11575-11590,11807-11840,11845-11851,11856-11874,11937-11964,12279-12326,12513-12577] NodeHostName=localhost Port=[30001-32048]
#PartitionName=test      Nodes=test0[1-2] Default=YES MaxTime=168:00:00 State=up  DefMemPerCPU=100 TRESBillingWeights="CPU=2.0,Mem=0.25G"
#PartitionName=test2             Nodes=test0[3-4]    MaxTime=168:00:00 State=up DefMemPerCPU=100 TRESBillingWeights="CPU=1.0,Mem=0.25G"


#PartitionName=test      Nodes=test01 Default=YES MaxTime=168:00:00 State=up  DefMemPerCPU=100
#xPartitionName=test2             Nodes=test0[3-4]    MaxTime=168:00:00 State=up DefMemPerCPU=100


#PartitionName=DEFAULT TRESBillingWeights=cpu=1,mem=0.125G,gres/gpu=16
#PartitionName=nersc Nodes=nid[02534-02555,02557-02575,02580-02582,02666,02869-02891,02896-02904,03447-03465,03611-03626,03720-03727,03732-03744,03909-03915,03920-03929,04109-04111,04116-04128,04256-04284,04372-04399,04576-04600,04645-04683,04688-04747,04756-04811,04816-04825,04914-04936,04967-05000,05147-05168,05336-05391,05396-05440,05442-05451,05456-05469,05509-05515,05524-05537,05722-05753,06372-06389,06456-06475,06480-06543,06548-06603,06608-06614,06629-06661,06798-06799,06804-06819,06889-06927,06932-06949,06953-06974,07008-07034,07129-07160,07481-07498,07724-07755,07761-07823,07828-07883,07888-07943,08169-08183,08349-08395,08400-08424,08480-08498,08574-08588,09239-09291,09296-09310,09402-09417,09452-09471,09605-09615,09620-09625,10288-10311,10646-10684,10938-10955,10960-10964,10971-10998,11003-11023,11028-11030,11068-11083,11088-11095,11210-11211,11216-11237,11324-11339,11344-11346,11363-11407,11412-11467,11472-11505,11575-11590,11807-11840,11845-11851,11856-11874,11937-11964,12279-12326,12513-12577]

#PartitionName=test1 Nodes=test[01-08] PriorityTier=1 Default=YES  MaxTime=500
PartitionName=test1 Nodes=test[01-08] PriorityTier=1 Default=YES  MaxTime=500
PartitionName=test2 Nodes=test02 PriorityTier=1 Default=YES  MaxTime=500
#PartitionName=test1 Nodes=test02 PriorityTier=1 Default=YES  MaxTime=500
#PartitionName=test2 Nodes=test[05-08] PriorityTier=1 Default=YES OverSubscribe=FORCE:1 MaxTime=20
#PartitionName=hipri Nodes=test[01-08] PriorityTier=2 OverSubscribe=FORCE:2

OverTimeLimit=2

GresTypes=gpu,tmpfs

PrologSlurmctld=/mnt/slurm/etc/prologSlurmctl.sh

######
#Power Save
#SuspendProgram=/mnt/slurm/etc/suspend.sh
#ResumeProgram=/mnt/slurm/etc/resume.sh
#SuspendTimeout=5
#ResumeTimeout=10
#ResumeRate=0
#SuspendRate=0
#SuspendExcNodes=teach[02,36,41]
#SuspendTime=10







MCSPlugin=mcs/account
MCSParameters=ondemand,select,privatedata
PrivateData=jobs,nodes
